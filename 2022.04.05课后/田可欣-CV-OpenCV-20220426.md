# 集成学习

Created: April 29, 2022 11:35 PM

上课时间: April 26, 2022

# 集成学习方法

## 目的

一定程度上提高预测精度

## [本质](https://blog.csdn.net/lan_faster/article/details/103200506)

集成学习方法是指组合多个模型，以获得更好的效果，即将多个弱学习器组合成一个强学习器，使集成的模型具有更强的[泛化能力](https://so.csdn.net/so/search?q=%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B&spm=1001.2101.3001.7020)

## 常见集成学习方法

- Stacking
  - 使用不同的模型
  - 将许多不同的模型一起使用
- Bagging
  - 每个数据集点同等几率采集
  - 最后通过投票方式选择
- Boosting
  - 样本分布与Bagging不同
  - ada boosting包含无数weak learner，组成一个strong learner

## 特点

- 与具体验证集划分联系紧密
- 如果**硬件设备不允许**建议选取留出法
- 如果需要**追求精度**可以使用交叉验证的方法

## 策略

- 平均
  - 简单平均
  - 加权平均
- 投票
  - 多数投票法
  - 加权投票法

## 深度学习中的集成学习

### Dropout

- 特点：
  - 在每个训练批次中，通过随机让一部分的节点停止工作
  - 同时，在预测的过程中让所有的节点都其作用
- 使用场景：CNN网络
- 优点：
  - 可以有效缓解模型过拟合的情况
  - 可以在预测时增加模型的精度

![7-Dropout.jpg](https://github.com/kk37111754/Aliyun-CV/blob/075c5d17f2091705b65c303b205f3ca565ed31bd/2022.04.05%E8%AF%BE%E5%90%8E/%E7%94%B0%E5%8F%AF%E6%AC%A3-%E8%B5%84%E6%96%99/pic/7-Dropout.jpg)

```python
# 卷积层
nn.Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2)),
# 激活函数
nn.ReLU(),
# 集成学习
nn.Dropout(0.25),
# 池化层
nn.MaxPool2d(2),
```

### TTA — Test Time Augmentation 测试集数据扩增

- 特点：
  - 对同一个样本预测三次
  - 对三次结果进行平均
- 使用场景：
  - 在训练时使用
  - 在预测时候进行数据扩增

### Snapshot

- 特点：
  - 使用**cyclical learning rate**进行训练模型，
  - 保存精度比较好的一些checkopint
  - 将多个checkpoint进行模型集成
- 优点：
  - 在一定程度上提高模型精度
- 缺点：
  - 需要更长的训练时间
